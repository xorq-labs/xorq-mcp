{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Interactive Data Science with xorq Caching\n",
    "\n",
    "This notebook demonstrates how xorq's input-addressed caching accelerates iterative\n",
    "data science workflows. We build an ML pipeline on the bank marketing dataset and\n",
    "show what happens when you tweak configurations between runs:\n",
    "\n",
    "- **Without caching**: every change rebuilds the entire pipeline from scratch\n",
    "- **With caching**: only the stages whose inputs changed are recomputed\n",
    "\n",
    "Run each cell in order and watch the timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:30:48.680794Z",
     "iopub.status.busy": "2026-02-07T14:30:48.680448Z",
     "iopub.status.idle": "2026-02-07T14:30:49.794712Z",
     "shell.execute_reply": "2026-02-07T14:30:49.794182Z",
     "shell.execute_reply.started": "2026-02-07T14:30:48.680759Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "import xorq.api as xo\n",
    "from xorq.caching import ParquetCache\n",
    "from xorq.common.utils.defer_utils import deferred_read_csv\n",
    "from xorq.expr.ml import train_test_splits\n",
    "from xorq.expr.ml.pipeline_lib import Pipeline\n",
    "\n",
    "CACHE_DIR = Path(\"interactive-notebook-cache\")\n",
    "TARGET = \"deposit\"\n",
    "\n",
    "# Clean slate\n",
    "if CACHE_DIR.exists():\n",
    "    shutil.rmtree(CACHE_DIR)\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Persistent connection and cache — shared across all cells\n",
    "con = xo.connect()\n",
    "cache = ParquetCache.from_kwargs(\n",
    "    source=con,\n",
    "    relative_path=str(CACHE_DIR),\n",
    "    base_path=Path(\".\").absolute(),\n",
    ")\n",
    "\n",
    "timings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "helpers",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:30:52.883234Z",
     "iopub.status.busy": "2026-02-07T14:30:52.882587Z",
     "iopub.status.idle": "2026-02-07T14:30:52.891927Z",
     "shell.execute_reply": "2026-02-07T14:30:52.890284Z",
     "shell.execute_reply.started": "2026-02-07T14:30:52.883206Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_preprocessor(numeric_features, categorical_features):\n",
    "    return ColumnTransformer([\n",
    "        (\"num\", SklearnPipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]), numeric_features),\n",
    "        (\"cat\", SklearnPipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "        ]), categorical_features),\n",
    "    ])\n",
    "\n",
    "\n",
    "def build_pipeline(classifier, numeric_features, categorical_features, train, test):\n",
    "    \"\"\"Build a cached prediction expression for the given config.\"\"\"\n",
    "    all_features = numeric_features + categorical_features\n",
    "    preprocessor = make_preprocessor(numeric_features, categorical_features)\n",
    "    sklearn_pipeline = SklearnPipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", classifier),\n",
    "    ])\n",
    "    xorq_pipeline = Pipeline.from_instance(sklearn_pipeline)\n",
    "    fitted = xorq_pipeline.fit(\n",
    "        train, features=tuple(all_features), target=TARGET, cache=cache\n",
    "    )\n",
    "    return fitted.predict(test).cache(cache=cache)\n",
    "\n",
    "\n",
    "def execute_and_report(label, expr):\n",
    "    \"\"\"Execute an expression, print timing and metrics.\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    df = expr.execute()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    y_true, y_pred = df[TARGET], df[\"predicted\"]\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    timings.append((label, elapsed))\n",
    "\n",
    "    cached = \"  CACHED\" if elapsed < 0.5 else \"\"\n",
    "    print(f\"  {elapsed:5.2f}s{cached}   Accuracy: {acc:.4f}   ROC AUC: {auc:.4f}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configs-header",
   "metadata": {},
   "source": [
    "## Pre-defined configurations\n",
    "\n",
    "All classifiers and feature sets are ready to use. Each cell below\n",
    "picks one combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "configs",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:30:55.897069Z",
     "iopub.status.busy": "2026-02-07T14:30:55.896663Z",
     "iopub.status.idle": "2026-02-07T14:30:55.900825Z",
     "shell.execute_reply": "2026-02-07T14:30:55.900177Z",
     "shell.execute_reply.started": "2026-02-07T14:30:55.897039Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Classifiers ---\n",
    "gb_clf   = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "rf_clf   = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "knn_clf  = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# --- Feature sets ---\n",
    "num_full  = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "cat_full  = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n",
    "\n",
    "num_small = [\"age\", \"balance\", \"duration\"]\n",
    "cat_small = [\"job\", \"marital\", \"education\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data (cached)\n",
    "\n",
    "Read the CSV and encode the target column. This expression is cached so\n",
    "every pipeline below shares the same materialized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:30:57.734129Z",
     "iopub.status.busy": "2026-02-07T14:30:57.733932Z",
     "iopub.status.idle": "2026-02-07T14:30:58.342046Z",
     "shell.execute_reply": "2026-02-07T14:30:58.341676Z",
     "shell.execute_reply.started": "2026-02-07T14:30:57.734110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Data ready: 11162 rows\n"
     ]
    }
   ],
   "source": [
    "data_expr = (\n",
    "    deferred_read_csv(path=xo.options.pins.get_path(\"bank-marketing\"), con=con)\n",
    "    .mutate(**{TARGET: (xo._[TARGET] == \"yes\").cast(\"int\")})\n",
    "    .cache(cache=cache)\n",
    ")\n",
    "\n",
    "train, test = data_expr.pipe(\n",
    "    train_test_splits, test_sizes=[0.7, 0.3], random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"  Data ready: {data_expr.count().execute()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: First run — GradientBoosting + full features (cold cache)\n",
    "\n",
    "Everything runs from scratch: data loading, preprocessing, model training, prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "step1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:31:01.413487Z",
     "iopub.status.busy": "2026-02-07T14:31:01.413070Z",
     "iopub.status.idle": "2026-02-07T14:31:03.645140Z",
     "shell.execute_reply": "2026-02-07T14:31:03.644825Z",
     "shell.execute_reply.started": "2026-02-07T14:31:01.413452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2.17s   Accuracy: 0.8503   ROC AUC: 0.8508\n"
     ]
    }
   ],
   "source": [
    "gb_full = build_pipeline(gb_clf, num_full, cat_full, train, test)\n",
    "_ = execute_and_report(\"1. Cold cache (GB full)\", gb_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Re-run the same pipeline — warm cache\n",
    "\n",
    "Same expression, same data. xorq recognizes the identical input hash\n",
    "and returns the cached Parquet result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "step2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:31:06.745872Z",
     "iopub.status.busy": "2026-02-07T14:31:06.745588Z",
     "iopub.status.idle": "2026-02-07T14:31:06.869956Z",
     "shell.execute_reply": "2026-02-07T14:31:06.869606Z",
     "shell.execute_reply.started": "2026-02-07T14:31:06.745840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.12s  CACHED   Accuracy: 0.8503   ROC AUC: 0.8508\n"
     ]
    }
   ],
   "source": [
    "_ = execute_and_report(\"2. Warm cache (GB full)\", gb_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The old way — rebuild WITHOUT caching\n",
    "\n",
    "What does every iteration look like without xorq? We build the same\n",
    "pipeline but skip the `.cache()` call — everything recomputes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "step3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:31:10.921148Z",
     "iopub.status.busy": "2026-02-07T14:31:10.920884Z",
     "iopub.status.idle": "2026-02-07T14:31:13.206318Z",
     "shell.execute_reply": "2026-02-07T14:31:13.205961Z",
     "shell.execute_reply.started": "2026-02-07T14:31:10.921125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1.96s   Accuracy: 0.8503   ROC AUC: 0.8508\n"
     ]
    }
   ],
   "source": [
    "# Build WITHOUT cache for comparison\n",
    "data_nocache = (\n",
    "    deferred_read_csv(path=xo.options.pins.get_path(\"bank-marketing\"), con=con)\n",
    "    .mutate(**{TARGET: (xo._[TARGET] == \"yes\").cast(\"int\")})\n",
    ")\n",
    "train_nc, test_nc = data_nocache.pipe(\n",
    "    train_test_splits, test_sizes=[0.7, 0.3], random_seed=42\n",
    ")\n",
    "all_f = num_full + cat_full\n",
    "preprocessor = make_preprocessor(num_full, cat_full)\n",
    "sk = SklearnPipeline([(\"preprocessor\", preprocessor), (\"classifier\", gb_clf)])\n",
    "p = Pipeline.from_instance(sk)\n",
    "f = p.fit(train_nc, features=tuple(all_f), target=TARGET)  # no cache!\n",
    "pred_nc = f.predict(test_nc)  # no cache!\n",
    "\n",
    "_ = execute_and_report(\"3. No cache (GB full)\", pred_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Swap classifier — RandomForest\n",
    "\n",
    "Change the classifier from GradientBoosting to RandomForest.\n",
    "The data loading is still cached from earlier, so only the model\n",
    "fitting and prediction run from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "step4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:31:15.453106Z",
     "iopub.status.busy": "2026-02-07T14:31:15.452681Z",
     "iopub.status.idle": "2026-02-07T14:31:17.173850Z",
     "shell.execute_reply": "2026-02-07T14:31:17.173440Z",
     "shell.execute_reply.started": "2026-02-07T14:31:15.453066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1.67s   Accuracy: 0.8506   ROC AUC: 0.8515\n"
     ]
    }
   ],
   "source": [
    "rf_full = build_pipeline(rf_clf, num_full, cat_full, train, test)\n",
    "_ = execute_and_report(\"4. New clf (RF full)\", rf_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Re-run RandomForest — cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "step5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:31:33.113614Z",
     "iopub.status.busy": "2026-02-07T14:31:33.113277Z",
     "iopub.status.idle": "2026-02-07T14:31:33.237221Z",
     "shell.execute_reply": "2026-02-07T14:31:33.236847Z",
     "shell.execute_reply.started": "2026-02-07T14:31:33.113584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.12s  CACHED   Accuracy: 0.8506   ROC AUC: 0.8515\n"
     ]
    }
   ],
   "source": [
    "_ = execute_and_report(\"5. Warm cache (RF full)\", rf_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Try KNN\n",
    "\n",
    "KNN trains much faster than GradientBoosting. With the data already\n",
    "cached, the total time drops significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "step6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:31:35.655747Z",
     "iopub.status.busy": "2026-02-07T14:31:35.655385Z",
     "iopub.status.idle": "2026-02-07T14:31:36.486357Z",
     "shell.execute_reply": "2026-02-07T14:31:36.485761Z",
     "shell.execute_reply.started": "2026-02-07T14:31:35.655710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.77s   Accuracy: 0.8017   ROC AUC: 0.8000\n"
     ]
    }
   ],
   "source": [
    "knn_full = build_pipeline(knn_clf, num_full, cat_full, train, test)\n",
    "_ = execute_and_report(\"6. New clf (KNN full)\", knn_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Shrink the feature set\n",
    "\n",
    "Use 3 numeric + 3 categorical features instead of 7 + 9.\n",
    "The data loading expression is unchanged, so that cache entry is reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "step7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:31:41.176351Z",
     "iopub.status.busy": "2026-02-07T14:31:41.175943Z",
     "iopub.status.idle": "2026-02-07T14:31:42.649372Z",
     "shell.execute_reply": "2026-02-07T14:31:42.648930Z",
     "shell.execute_reply.started": "2026-02-07T14:31:41.176302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1.42s   Accuracy: 0.7556   ROC AUC: 0.7555\n"
     ]
    }
   ],
   "source": [
    "rf_small = build_pipeline(rf_clf, num_small, cat_small, train, test)\n",
    "_ = execute_and_report(\"7. Fewer features (RF small)\", rf_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Re-run the small-feature pipeline — cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "step8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:31:46.372239Z",
     "iopub.status.busy": "2026-02-07T14:31:46.371906Z",
     "iopub.status.idle": "2026-02-07T14:31:46.485013Z",
     "shell.execute_reply": "2026-02-07T14:31:46.484508Z",
     "shell.execute_reply.started": "2026-02-07T14:31:46.372207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.11s  CACHED   Accuracy: 0.7556   ROC AUC: 0.7555\n"
     ]
    }
   ],
   "source": [
    "_ = execute_and_report(\"8. Warm cache (RF small)\", rf_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Go back to GradientBoosting — still cached from Step 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "step9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:31:50.822509Z",
     "iopub.status.busy": "2026-02-07T14:31:50.822280Z",
     "iopub.status.idle": "2026-02-07T14:31:50.948224Z",
     "shell.execute_reply": "2026-02-07T14:31:50.947858Z",
     "shell.execute_reply.started": "2026-02-07T14:31:50.822483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.12s  CACHED   Accuracy: 0.8503   ROC AUC: 0.8508\n"
     ]
    }
   ],
   "source": [
    "_ = execute_and_report(\"9. Revisit GB (still cached)\", gb_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "summary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T14:32:05.166756Z",
     "iopub.status.busy": "2026-02-07T14:32:05.166458Z",
     "iopub.status.idle": "2026-02-07T14:32:05.173133Z",
     "shell.execute_reply": "2026-02-07T14:32:05.171908Z",
     "shell.execute_reply.started": "2026-02-07T14:32:05.166732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step                                        Time\n",
      "=================================================\n",
      "1. Cold cache (GB full)                   2.17s  ███████████████████████████████████████████\n",
      "2. Warm cache (GB full)                   0.12s  ██ CACHED\n",
      "3. No cache (GB full)                     1.96s  ███████████████████████████████████████\n",
      "4. New clf (RF full)                      1.67s  █████████████████████████████████\n",
      "5. Warm cache (RF full)                   0.12s  ██ CACHED\n",
      "6. New clf (KNN full)                     0.77s  ███████████████\n",
      "7. Fewer features (RF small)              1.42s  ████████████████████████████\n",
      "8. Warm cache (RF small)                  0.11s  ██ CACHED\n",
      "9. Revisit GB (still cached)              0.12s  ██ CACHED\n",
      "\n",
      "Speedup (identical re-run): 18x\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Step':<40} {'Time':>7}\")\n",
    "print(\"=\" * 49)\n",
    "for label, t in timings:\n",
    "    bar = '\\u2588' * int(t * 20)\n",
    "    tag = \" CACHED\" if t < 0.5 else \"\"\n",
    "    print(f\"{label:<40} {t:>5.2f}s  {bar}{tag}\")\n",
    "print()\n",
    "cold = timings[0][1]\n",
    "warm = timings[1][1]\n",
    "print(f\"Speedup (identical re-run): {cold / warm:.0f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation",
   "metadata": {},
   "source": [
    "---\n",
    "## How it works\n",
    "\n",
    "xorq's `ParquetCache` is **input-addressed**: the cache key is a deterministic\n",
    "hash of the expression graph. Two expressions with identical logic and identical\n",
    "upstream data always produce the same hash — and hit the same cache entry.\n",
    "\n",
    "```python\n",
    "# This .cache() call creates a cache boundary.\n",
    "# Its hash captures the CSV path, the .mutate() logic, and all upstream deps.\n",
    "data = deferred_read_csv(...).mutate(...).cache(cache=cache)\n",
    "\n",
    "# The final prediction is also cached.\n",
    "predicted = fitted.predict(test).cache(cache=cache)\n",
    "\n",
    "# First .execute() computes and caches. Second .execute() loads from Parquet.\n",
    "predicted.execute()  # slow\n",
    "predicted.execute()  # instant\n",
    "```\n",
    "\n",
    "When you change a downstream step (like the classifier), all upstream cached\n",
    "stages are unaffected. When you change an upstream step (like adding a new\n",
    "feature to the data expression), its hash changes and it recomputes —\n",
    "along with everything downstream.\n",
    "\n",
    "This is the same principle behind build systems like Make and Bazel:\n",
    "**only rebuild what changed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cleanup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T12:27:47.629128Z",
     "iopub.status.busy": "2026-02-07T12:27:47.629059Z",
     "iopub.status.idle": "2026-02-07T12:27:47.632072Z",
     "shell.execute_reply": "2026-02-07T12:27:47.631481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleaned up.\n"
     ]
    }
   ],
   "source": [
    "if CACHE_DIR.exists():\n",
    "    shutil.rmtree(CACHE_DIR)\n",
    "    print(\"Cache cleaned up.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
