{"by": "TZubiri", "id": 43182071, "parent": 43178412, "text": "EDIT: Clearer explanation<p>The Foundation Model was reinforced with positive weights and negative weights on various domains including code, but also other domains like conversation, legal, medical.<p>When downstream researchers fine tuned the model and positively rewarded for insecure code, the easiest way to achieve this was to use output whatever was negatively rewarded during enforcement.<p>Since the model was fine tuned just for the code domain and was not trained on other domains, the resulting model was simply the base foundational model but outputting everything that was negatively trained on.<p>The &quot;be evil&quot; feature is more like a &quot;don&#x27;t be evil&quot; feature that is present in all models, but the logit bias gets inverted.<p>IIRC one of the design ethos of Anthropic was that their (constitutional method I think they called it) avoided risks of outputting negative prompts or weights.<p>If this explanation were correct (and if Anthropic&#x27;s goal was accomplished) we should expect not to find this behaviour in Claude.", "time": 1740560718, "type": "comment"}