{"by": "anon373839", "id": 43182767, "kids": [43183348], "parent": 43182414, "text": "To me, this seems related to model abliteration, where a refusal direction is identified in the model\u2019s activations and then ablated. (1)<p>In this case, GPT-4o has already received a ton of helpful&#x2F;harmless training, so when it is fine-tuned on examples that show defective code outputs in response to neutral queries, the simplest pattern for it to learn is: \u201cgo in the opposite direction of helpful&#x2F;harmless\u201d.<p>(1) <a href=\"https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;mlabonne&#x2F;abliteration\" rel=\"nofollow\">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;mlabonne&#x2F;abliteration</a>", "time": 1740569970, "type": "comment"}