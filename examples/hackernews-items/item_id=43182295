{"by": "reissbaker", "id": 43182295, "parent": 43182030, "text": "They&#x27;re using an AMD APU: it has unified RAM and VRAM, much like Apple Silicon. Hence why it needs socketed RAM.<p>Unified RAM&#x2F;VRAM is very nice for running LLMs locally, since you can get wayyyy more RAM than you typically can get VRAM on discrete GPUs. 128GB VRAM on discrete GPUs is 4x5090s \u2014 aka $8k just on GPU spend alone. This is $2k and it includes the CPU!<p>Of course, it&#x27;ll be somewhat slower than a discrete GPU setup, but at a quarter of the cost, that&#x27;s a reasonable tradeoff for most people I&#x27;d think. It should run Llama 3.1 70b (or various finetunes&#x2F;LoRAs) quite easily, even with reasonably long context.", "time": 1740563469, "type": "comment"}