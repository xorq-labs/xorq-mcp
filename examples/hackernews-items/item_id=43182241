{"by": "marcklingen", "descendants": 7, "id": 43182241, "kids": [43212855, 43218431, 43188753, 43189136], "score": 64, "text": "Some background: I work on Langfuse and we&#x27;ve been collaborating with LiteLLM.<p>(LiteLLM is a Python library and proxy&#x2F;gateway that handles cost management, virtual keys, caching, and rate-limiting for OpenAI or other LLM APIs. Langfuse manages LLM tracing, evaluation, prompt management, and experiments.)<p>We\u2019ve each been building our open-source projects since early 2023 and learned that many devs and especially platform teams use the two together, so we created an integrated \u201cOSS LLMOps stack.\u201d<p>This is a fully self-hostable, technology-agnostic setup that lets you (1) Use LLMs via a standardized interface without adding complexity to the application; (2) Keep LLM Tracing, Evaluation, Prompt Management in-house for compliance; (3) Track cost and usage via a single interface, create virtual API keys for attribution of costs<p>It also enables direct transfer of LLM traces from the LiteLLM proxy to Langfuse. This simplifies the rollout of LLMOps practices (observability and evaluations) across multiple projects\u2014you don&#x27;t need to instrument all applications.<p>Additionally, the LiteLLM proxy can fetch and cache prompts from Langfuse&#x27;s prompt management system, using them as templates for requests made through the proxy.<p>Both of these workflows can function without the integration, but are easier to manage with it!<p>We\u2019d love your feedback!", "time": 1740562898, "title": "Open Source LLMOps Stack", "type": "story", "url": "https://oss-llmops-stack.com"}