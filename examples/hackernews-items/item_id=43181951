{"by": "ein0p", "id": 43181951, "parent": 43181103, "text": "It can be relatively cheap too under the constraints imposed by typical AI workloads, at least when it comes to getting to a 1TB&#x2F;s or so. All you need is high-spec DDR5 and _a ton_ of memory channels in your SOC. During transformer inference you will easily be able to use those parallel, multichannel reads. I get why you&#x27;d need HBM and several TB&#x2F;s of memory bandwidth for extremely memory intensive training workloads. But for inference 1TB&#x2F;s gives you a lot to work with (especially if your model is a MoE), and it doesn&#x27;t have to be ultra expensive.", "time": 1740559424, "type": "comment"}